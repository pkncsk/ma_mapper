#%%
import numpy as np
import pandas as pd
import os
import sys
from concurrent.futures import ProcessPoolExecutor
from itertools import repeat 
from Bio.AlignIO import MafIO
from . import logger
if sys.version_info >= (3, 8, 0):
    from typing import Literal, Tuple, List
else:
    from typing_extensions import Literal, Tuple, List
#%%
_AGEARG = Literal[None,'calibrate']
_COUNTARG = Literal['human_ref','coverage','common', 'common_raw', 'total_raw','a','t','c','g','base_freq']
def extract_maf(internal_id:str,
                maf_file:str, 
                chrom:str, 
                start:int, 
                end:int, 
                strand:str,
                target_species:str = 'Homo_sapiens', 
                count_arg:_COUNTARG = 'common',
                e_value_df:pd.DataFrame = None, 
                internal_id_df:pd.DataFrame = None,
                ):
    #print(internal_id)
    #print(target_species,chrom, start, end, strand)
    maf_id = f'{target_species}.{chrom}'
    
    index_maf = MafIO.MafIndex(f'{maf_file}.mafindex', maf_file, maf_id) 
    n_strand = -1 if strand == '-' else 1
    results =index_maf.get_spliced(start,end,n_strand)
    if e_value_df is None and internal_id_tbl is None:
        collector = {seqrec.id.split('.')[0]: np.array(list(seqrec.seq.upper())) for seqrec in results}
    else:
        if internal_id_df is not None:
            subfamily = str.split(internal_id, sep ='_')[:-1]
            internal_id_idx = str.split(internal_id, sep ='_')[-1]

            _internal_id = internal_id_df[internal_id_df.index==int(internal_id_idx)]['internal_id'].values[0]
        else:
            _internal_id = internal_id
        e_value_internal_id=e_value_df[e_value_df.internal_id == _internal_id]
        if e_value_internal_id.shape[0] <= 1:
            collector = {seqrec.id.split('.')[0]: np.array(list(seqrec.seq.upper())) for seqrec in results if seqrec.id.split('.')[0] == 'Homo_sapiens'}
        else:
            try:
                collector = {seqrec.id.split('.')[0]: np.array(list(seqrec.seq.upper())) for seqrec in results if seqrec.id.split('.')[0] in e_value_internal_id.species.values}
            except KeyError:
                print(f'source: {internal_id}')

    
    array_transposed=np.array(list(collector.values())).transpose()
    output_array=[]
    try: 
        ref_alleles=collector['Homo_sapiens']
    except KeyError:
                print(f'source: {internal_id}')
    for idx,ref_pos in enumerate(array_transposed):
        (unique, counts) = np.unique(ref_pos, return_counts=True)
        frequencies = dict(zip(unique, counts))
        ref_allele=ref_alleles[idx]
        #frequencies.pop('-', None) #->count deletion/insertion
        total = sum(frequencies.values())
        if count_arg == 'human_ref':
            alt_count = total - frequencies[ref_allele]
            alt_freq=alt_count/total
            if total == 1:
                alt_freq = np.nan
            output_array.append(alt_freq)
        elif count_arg in ['coverage', 'total_raw']:
            output_array.append(total)
        elif count_arg in ['common', 'common_raw']:
            common_allele = max(frequencies, key=frequencies.get)
            common_count = frequencies.get(common_allele)
            if count_arg == 'common':
                common_freq = common_count/total
                if total == 1:
                    alt_freq = np.nan
                output_array.append(common_freq)
            else:
                output_array.append(common_count)
        elif count_arg in ['A','T','C','G']:
            nucl_count = frequencies.get(count_arg, 0)
            output_array.append(nucl_count)
        elif count_arg == 'base_count':
            output_array.append(frequencies)
    if isinstance(output_array, str):
        print('debug',internal_id,output_array)
    return output_array
#%%
def maf_io(metadata: pd.DataFrame|str, 
           maf:str,
           output_dir:str|None = None, 
           separated_maf:bool = False, 
           target_species:str = 'Homo_sapiens', 
           count_arg:_COUNTARG = 'common', 
           save_to_file:bool = False, 
           custom_id:bool = False, 
           custom_prefix='entry', 
           e_value_table: str|pd.DataFrame|None=None, 
           internal_id_table: str|pd.DataFrame|None=None, **kwargs):
    if isinstance(metadata, str):
        if os.path.isfile(metadata):
            metadata_local = pd.read_csv(metadata, sep='\t', header=None)
        else:
            logger.error('metadata file not found')
    else:
        metadata_local = metadata
    if output_dir is None:
        if isinstance(metadata, str):
            output_dir = '/'.join(str.split(metadata, sep ='/')[:-1])
        else: 
            output_dir = os.path.dirname(os.path.abspath(__file__))

    logger.info(f'extract from maf target: {maf}')
    if custom_id == False:
        meta_id = [f'{custom_prefix}_{index}' for index in metadata_local.index.astype(str)]
        metadata_local['meta_id'] = meta_id
    else:
        metadata_local['meta_id'] = metadata_local.iloc[:,3]
        meta_id = metadata_local.meta_id.unique()

    grouped = metadata_local.groupby('meta_id', sort=False)
    chrom_list = grouped.apply(lambda x: x.iloc[:,0].unique()[0], include_groups=False).tolist()
    start_list = grouped.apply(lambda x: x.iloc[:,1].tolist(), include_groups=False).tolist()
    end_list = grouped.apply(lambda x: x.iloc[:,2].tolist(), include_groups=False).tolist()
    strand_list = grouped.apply(lambda x: x.iloc[:,5].unique()[0], include_groups=False).tolist()
    maf_call_list = []
    for chrom in chrom_list:
        if separated_maf == True:
            maf_file = f'{maf}.{chrom}'
        else:
            maf_file = maf
        maf_call_list.append(maf_file)

    if e_value_table is None:
        logger.info('normal maf extraction without calibration')
        with ProcessPoolExecutor(max_workers=40) as executor:
            results  = executor.map(extract_maf,meta_id, maf_call_list, chrom_list, start_list, end_list, strand_list, repeat(target_species) ,repeat(count_arg))
    elif internal_id_table is None:
        logger.info('calibrate age using e-value')
        if isinstance(e_value_table, str):
            e_value_df = pd.read_csv(e_value_table, sep='\t')
        elif isinstance(e_value_table, pd.DataFrame):
            e_value_df = e_value_table
        with ProcessPoolExecutor(max_workers=40) as executor:
            results  = executor.map(extract_maf,meta_id, maf_call_list, chrom_list, start_list, end_list, strand_list, repeat(target_species) ,repeat(count_arg), repeat(e_value_df))
    else:
        logger.info('calibrate age using e-value and internal_id conversion')
        if isinstance(e_value_table, str):
            e_value_df = pd.read_csv(e_value_table, sep='\t')
        elif isinstance(e_value_table, pd.DataFrame):
            e_value_df = e_value_table
        if isinstance(internal_id_table, str):
            internal_id_df = pd.read_csv(internal_id_table, sep='\t')
        elif isinstance(internal_id_table, pd.DataFrame):
            internal_id_df = internal_id_table
        with ProcessPoolExecutor(max_workers=40) as executor:
            results  = executor.map(extract_maf,meta_id, maf_call_list, chrom_list, start_list, end_list, strand_list, repeat(target_species) ,repeat(count_arg), repeat(e_value_df), repeat(internal_id_df))
    maf_out = []
    for result in results:
        maf_out.append(result)
    if save_to_file == True:
        import compress_pickle
        output_filepath = f'{output_dir}/maf_out.lzma'
        compress_pickle.dump(maf_out, output_filepath, compression="lzma")
        logger.info('done, saving maf_out at: ', output_filepath)
    else:
        logger.info('done, returning maf_out as object')
        return maf_out
#%%
